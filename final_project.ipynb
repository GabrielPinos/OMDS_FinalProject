{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86ef8b0b",
   "metadata": {},
   "source": [
    "# **FINAL PROJECT _ Optimization Methods for Data Science** \n",
    "### A.A. 2024-2025\n",
    "\n",
    "**Pinos Gabriel** - 1965035  \n",
    "**Federico Lattanzio** - 1886519\n",
    "\n",
    "pinos.1965035@studenti.uniroma1.it  \n",
    "lattanzio.1886519@studenti.uniroma1.it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c500a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from warnings import filterwarnings\n",
    "import matplotlib.pyplot as plt\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6f0d8d",
   "metadata": {},
   "source": [
    "## **Dataset overview**\n",
    "--------\n",
    "**Read all the 3 datasets and take a look to them**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695bd8b3",
   "metadata": {},
   "source": [
    "The UTKFace-derived datasets contain features extracted via a ResNet backbone.\n",
    "Each dataset includes:\n",
    "\n",
    "- A number of columns labeled `feat_i`, which are the ResNet features.\n",
    "- A final column `gt`, which is the ground truth label:\n",
    "  - `AGE_PREDICTION.csv`: age values (float, from 0 to 100)\n",
    "  - `GENDER_CLASSIFICATION.csv`: binary (0 = Female, 1 = Male)\n",
    "  - `ETHNICITY_CLASSIFICATION.csv`: integer from 0 to 4 (5 ethnicity classes)\n",
    "\n",
    "We will first explore the structure and content of the datasets to understand their dimensions and distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba24d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_df = pd.read_csv(\"dataset\\AGE_PREDICTION.csv\")\n",
    "ethnicity_df = pd.read_csv(\"dataset\\ETHNICITY_CLASSIFICATION.csv\")\n",
    "gender_df = pd.read_csv(\"dataset\\GENDER_CLASSIFICATION.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75487041",
   "metadata": {},
   "source": [
    "We begin by inspecting the first few rows and the label distribution of the `AGE_PREDICTION.csv` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "843681f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 entries in AGE_PREDICTION dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>feat_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_24</th>\n",
       "      <th>feat_25</th>\n",
       "      <th>feat_26</th>\n",
       "      <th>feat_27</th>\n",
       "      <th>feat_28</th>\n",
       "      <th>feat_29</th>\n",
       "      <th>feat_30</th>\n",
       "      <th>feat_31</th>\n",
       "      <th>feat_32</th>\n",
       "      <th>gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.686191</td>\n",
       "      <td>-0.989465</td>\n",
       "      <td>-0.920503</td>\n",
       "      <td>1.607427</td>\n",
       "      <td>-0.896248</td>\n",
       "      <td>1.118974</td>\n",
       "      <td>-0.969456</td>\n",
       "      <td>1.811707</td>\n",
       "      <td>2.560955</td>\n",
       "      <td>3.803463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.862891</td>\n",
       "      <td>-0.909545</td>\n",
       "      <td>-0.915361</td>\n",
       "      <td>-0.952061</td>\n",
       "      <td>-0.989461</td>\n",
       "      <td>1.911855</td>\n",
       "      <td>1.409705</td>\n",
       "      <td>2.303997</td>\n",
       "      <td>-0.981840</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.887917</td>\n",
       "      <td>4.915272</td>\n",
       "      <td>-0.939446</td>\n",
       "      <td>-0.343677</td>\n",
       "      <td>-0.964685</td>\n",
       "      <td>-0.478649</td>\n",
       "      <td>4.342395</td>\n",
       "      <td>-0.332870</td>\n",
       "      <td>-0.768041</td>\n",
       "      <td>-0.815375</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.939201</td>\n",
       "      <td>-0.965917</td>\n",
       "      <td>-0.969461</td>\n",
       "      <td>-0.934799</td>\n",
       "      <td>5.304822</td>\n",
       "      <td>0.934790</td>\n",
       "      <td>-0.410701</td>\n",
       "      <td>0.284690</td>\n",
       "      <td>4.919212</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.923215</td>\n",
       "      <td>2.746968</td>\n",
       "      <td>-0.918085</td>\n",
       "      <td>0.047804</td>\n",
       "      <td>-0.908587</td>\n",
       "      <td>-0.451752</td>\n",
       "      <td>2.984481</td>\n",
       "      <td>0.535007</td>\n",
       "      <td>-0.591029</td>\n",
       "      <td>-0.324043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.809726</td>\n",
       "      <td>-0.929934</td>\n",
       "      <td>-0.891814</td>\n",
       "      <td>-0.881796</td>\n",
       "      <td>3.415373</td>\n",
       "      <td>1.044108</td>\n",
       "      <td>-0.442615</td>\n",
       "      <td>0.033648</td>\n",
       "      <td>2.628199</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.268866</td>\n",
       "      <td>-0.408416</td>\n",
       "      <td>-0.935145</td>\n",
       "      <td>0.731800</td>\n",
       "      <td>-0.922438</td>\n",
       "      <td>0.221781</td>\n",
       "      <td>-0.046606</td>\n",
       "      <td>1.149634</td>\n",
       "      <td>0.592136</td>\n",
       "      <td>1.357959</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.834968</td>\n",
       "      <td>-0.937475</td>\n",
       "      <td>-0.917737</td>\n",
       "      <td>-0.929519</td>\n",
       "      <td>-0.226282</td>\n",
       "      <td>1.608048</td>\n",
       "      <td>0.276169</td>\n",
       "      <td>1.246468</td>\n",
       "      <td>-0.363367</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.529231</td>\n",
       "      <td>-0.829957</td>\n",
       "      <td>-0.897425</td>\n",
       "      <td>0.921280</td>\n",
       "      <td>-0.865304</td>\n",
       "      <td>0.331018</td>\n",
       "      <td>-0.644940</td>\n",
       "      <td>1.296097</td>\n",
       "      <td>1.166863</td>\n",
       "      <td>2.036034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.775411</td>\n",
       "      <td>-0.881967</td>\n",
       "      <td>-0.864018</td>\n",
       "      <td>-0.908001</td>\n",
       "      <td>-0.784495</td>\n",
       "      <td>1.329586</td>\n",
       "      <td>0.547925</td>\n",
       "      <td>1.195395</td>\n",
       "      <td>-0.810089</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.793087</td>\n",
       "      <td>1.832985</td>\n",
       "      <td>-0.964968</td>\n",
       "      <td>0.274521</td>\n",
       "      <td>-0.962229</td>\n",
       "      <td>-0.288595</td>\n",
       "      <td>2.333454</td>\n",
       "      <td>0.788157</td>\n",
       "      <td>-0.311396</td>\n",
       "      <td>0.353214</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.923235</td>\n",
       "      <td>-0.969646</td>\n",
       "      <td>-0.970492</td>\n",
       "      <td>-0.961975</td>\n",
       "      <td>2.462071</td>\n",
       "      <td>1.539259</td>\n",
       "      <td>-0.132857</td>\n",
       "      <td>0.922628</td>\n",
       "      <td>2.249941</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.627914</td>\n",
       "      <td>0.537031</td>\n",
       "      <td>-0.846287</td>\n",
       "      <td>0.484149</td>\n",
       "      <td>-0.802970</td>\n",
       "      <td>-0.255616</td>\n",
       "      <td>1.103370</td>\n",
       "      <td>0.905729</td>\n",
       "      <td>0.121083</td>\n",
       "      <td>0.820827</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.712115</td>\n",
       "      <td>-0.826480</td>\n",
       "      <td>-0.794454</td>\n",
       "      <td>-0.831328</td>\n",
       "      <td>1.042797</td>\n",
       "      <td>0.713609</td>\n",
       "      <td>-0.003151</td>\n",
       "      <td>0.097357</td>\n",
       "      <td>0.469993</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.825429</td>\n",
       "      <td>3.686025</td>\n",
       "      <td>-0.979020</td>\n",
       "      <td>0.053158</td>\n",
       "      <td>-0.983342</td>\n",
       "      <td>-0.448615</td>\n",
       "      <td>3.839871</td>\n",
       "      <td>0.161758</td>\n",
       "      <td>-0.641629</td>\n",
       "      <td>-0.414449</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.970626</td>\n",
       "      <td>-0.987172</td>\n",
       "      <td>-0.992049</td>\n",
       "      <td>-0.977634</td>\n",
       "      <td>4.310089</td>\n",
       "      <td>1.683480</td>\n",
       "      <td>-0.217012</td>\n",
       "      <td>1.010501</td>\n",
       "      <td>4.100075</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.978345</td>\n",
       "      <td>-0.941560</td>\n",
       "      <td>-0.906123</td>\n",
       "      <td>1.604776</td>\n",
       "      <td>-0.881297</td>\n",
       "      <td>0.819134</td>\n",
       "      <td>-0.879738</td>\n",
       "      <td>1.500830</td>\n",
       "      <td>1.694242</td>\n",
       "      <td>3.005683</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.845833</td>\n",
       "      <td>-0.906470</td>\n",
       "      <td>-0.917842</td>\n",
       "      <td>-0.910915</td>\n",
       "      <td>-0.926480</td>\n",
       "      <td>1.689097</td>\n",
       "      <td>1.170266</td>\n",
       "      <td>1.745933</td>\n",
       "      <td>-0.939529</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.975595</td>\n",
       "      <td>-0.999944</td>\n",
       "      <td>-0.979416</td>\n",
       "      <td>5.483491</td>\n",
       "      <td>-0.976307</td>\n",
       "      <td>3.397312</td>\n",
       "      <td>-0.999714</td>\n",
       "      <td>2.934085</td>\n",
       "      <td>6.361970</td>\n",
       "      <td>9.293163</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.980985</td>\n",
       "      <td>-0.976980</td>\n",
       "      <td>-0.982636</td>\n",
       "      <td>-0.989800</td>\n",
       "      <td>-0.999934</td>\n",
       "      <td>5.141039</td>\n",
       "      <td>4.198592</td>\n",
       "      <td>6.156951</td>\n",
       "      <td>-0.999915</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feat_1    feat_2    feat_3    feat_4    feat_5    feat_6    feat_7  \\\n",
       "0  2.686191 -0.989465 -0.920503  1.607427 -0.896248  1.118974 -0.969456   \n",
       "1 -0.887917  4.915272 -0.939446 -0.343677 -0.964685 -0.478649  4.342395   \n",
       "2 -0.923215  2.746968 -0.918085  0.047804 -0.908587 -0.451752  2.984481   \n",
       "3 -0.268866 -0.408416 -0.935145  0.731800 -0.922438  0.221781 -0.046606   \n",
       "4  0.529231 -0.829957 -0.897425  0.921280 -0.865304  0.331018 -0.644940   \n",
       "5 -0.793087  1.832985 -0.964968  0.274521 -0.962229 -0.288595  2.333454   \n",
       "6 -0.627914  0.537031 -0.846287  0.484149 -0.802970 -0.255616  1.103370   \n",
       "7 -0.825429  3.686025 -0.979020  0.053158 -0.983342 -0.448615  3.839871   \n",
       "8  1.978345 -0.941560 -0.906123  1.604776 -0.881297  0.819134 -0.879738   \n",
       "9  6.975595 -0.999944 -0.979416  5.483491 -0.976307  3.397312 -0.999714   \n",
       "\n",
       "     feat_8    feat_9   feat_10  ...   feat_24   feat_25   feat_26   feat_27  \\\n",
       "0  1.811707  2.560955  3.803463  ... -0.862891 -0.909545 -0.915361 -0.952061   \n",
       "1 -0.332870 -0.768041 -0.815375  ... -0.939201 -0.965917 -0.969461 -0.934799   \n",
       "2  0.535007 -0.591029 -0.324043  ... -0.809726 -0.929934 -0.891814 -0.881796   \n",
       "3  1.149634  0.592136  1.357959  ... -0.834968 -0.937475 -0.917737 -0.929519   \n",
       "4  1.296097  1.166863  2.036034  ... -0.775411 -0.881967 -0.864018 -0.908001   \n",
       "5  0.788157 -0.311396  0.353214  ... -0.923235 -0.969646 -0.970492 -0.961975   \n",
       "6  0.905729  0.121083  0.820827  ... -0.712115 -0.826480 -0.794454 -0.831328   \n",
       "7  0.161758 -0.641629 -0.414449  ... -0.970626 -0.987172 -0.992049 -0.977634   \n",
       "8  1.500830  1.694242  3.005683  ... -0.845833 -0.906470 -0.917842 -0.910915   \n",
       "9  2.934085  6.361970  9.293163  ... -0.980985 -0.976980 -0.982636 -0.989800   \n",
       "\n",
       "    feat_28   feat_29   feat_30   feat_31   feat_32  gt  \n",
       "0 -0.989461  1.911855  1.409705  2.303997 -0.981840  54  \n",
       "1  5.304822  0.934790 -0.410701  0.284690  4.919212  18  \n",
       "2  3.415373  1.044108 -0.442615  0.033648  2.628199  26  \n",
       "3 -0.226282  1.608048  0.276169  1.246468 -0.363367  33  \n",
       "4 -0.784495  1.329586  0.547925  1.195395 -0.810089  35  \n",
       "5  2.462071  1.539259 -0.132857  0.922628  2.249941  29  \n",
       "6  1.042797  0.713609 -0.003151  0.097357  0.469993  13  \n",
       "7  4.310089  1.683480 -0.217012  1.010501  4.100075  28  \n",
       "8 -0.926480  1.689097  1.170266  1.745933 -0.939529  43  \n",
       "9 -0.999934  5.141039  4.198592  6.156951 -0.999915  54  \n",
       "\n",
       "[10 rows x 33 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Total samples: 20475\n",
      " -  Feature columns: 32 (excluding target 'gt')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preview\n",
    "print(\"First 10 entries in AGE_PREDICTION dataset:\")\n",
    "display(age_df.head(10))\n",
    "\n",
    "# Dataset size\n",
    "print(f\"\\n - Total samples: {len(age_df)}\")\n",
    "print(f\" -  Feature columns: {age_df.shape[1] - 1} (excluding target 'gt')\")\n",
    "\n",
    "# Improved histogram plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(age_df['gt'], bins=30, edgecolor='white', color='#4A90E2', alpha=0.85)\n",
    "plt.title(\"Age Distribution\", fontsize=16)\n",
    "plt.xlabel(\"Age\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3781331",
   "metadata": {},
   "source": [
    "We now apply the same inspection to the `GENDER_CLASSIFICATION.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7d9d383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 entries in GENDER_CLASSIFICATION dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>feat_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_24</th>\n",
       "      <th>feat_25</th>\n",
       "      <th>feat_26</th>\n",
       "      <th>feat_27</th>\n",
       "      <th>feat_28</th>\n",
       "      <th>feat_29</th>\n",
       "      <th>feat_30</th>\n",
       "      <th>feat_31</th>\n",
       "      <th>feat_32</th>\n",
       "      <th>gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.900846</td>\n",
       "      <td>0.102587</td>\n",
       "      <td>-0.397814</td>\n",
       "      <td>0.112796</td>\n",
       "      <td>2.588096</td>\n",
       "      <td>-0.192754</td>\n",
       "      <td>-0.968311</td>\n",
       "      <td>-0.490886</td>\n",
       "      <td>-0.872099</td>\n",
       "      <td>-0.288411</td>\n",
       "      <td>...</td>\n",
       "      <td>2.541431</td>\n",
       "      <td>1.739102</td>\n",
       "      <td>0.166066</td>\n",
       "      <td>4.584869</td>\n",
       "      <td>-0.107031</td>\n",
       "      <td>-0.913990</td>\n",
       "      <td>-0.686416</td>\n",
       "      <td>-0.368085</td>\n",
       "      <td>-0.870545</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.838868</td>\n",
       "      <td>0.039976</td>\n",
       "      <td>-0.387101</td>\n",
       "      <td>0.055413</td>\n",
       "      <td>2.066874</td>\n",
       "      <td>-0.226948</td>\n",
       "      <td>-0.947416</td>\n",
       "      <td>-0.472817</td>\n",
       "      <td>-0.855387</td>\n",
       "      <td>-0.207101</td>\n",
       "      <td>...</td>\n",
       "      <td>1.991721</td>\n",
       "      <td>1.259745</td>\n",
       "      <td>0.065058</td>\n",
       "      <td>3.019790</td>\n",
       "      <td>-0.110633</td>\n",
       "      <td>-0.890023</td>\n",
       "      <td>-0.611625</td>\n",
       "      <td>-0.298235</td>\n",
       "      <td>-0.855208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.814961</td>\n",
       "      <td>-0.010184</td>\n",
       "      <td>-0.397147</td>\n",
       "      <td>0.092713</td>\n",
       "      <td>1.897454</td>\n",
       "      <td>-0.269387</td>\n",
       "      <td>-0.945285</td>\n",
       "      <td>-0.449579</td>\n",
       "      <td>-0.849705</td>\n",
       "      <td>-0.151179</td>\n",
       "      <td>...</td>\n",
       "      <td>1.822978</td>\n",
       "      <td>1.105511</td>\n",
       "      <td>0.065353</td>\n",
       "      <td>2.500681</td>\n",
       "      <td>-0.052730</td>\n",
       "      <td>-0.885691</td>\n",
       "      <td>-0.583346</td>\n",
       "      <td>-0.218140</td>\n",
       "      <td>-0.856456</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.110470</td>\n",
       "      <td>0.027849</td>\n",
       "      <td>-0.044310</td>\n",
       "      <td>-0.005343</td>\n",
       "      <td>0.177831</td>\n",
       "      <td>-0.232092</td>\n",
       "      <td>-0.562700</td>\n",
       "      <td>-0.400713</td>\n",
       "      <td>-0.552356</td>\n",
       "      <td>0.037349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.098367</td>\n",
       "      <td>-0.370318</td>\n",
       "      <td>-0.123008</td>\n",
       "      <td>-0.861314</td>\n",
       "      <td>0.106840</td>\n",
       "      <td>-0.483669</td>\n",
       "      <td>-0.224164</td>\n",
       "      <td>0.147321</td>\n",
       "      <td>-0.615051</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.626313</td>\n",
       "      <td>-0.091985</td>\n",
       "      <td>-0.373756</td>\n",
       "      <td>-0.005083</td>\n",
       "      <td>1.172486</td>\n",
       "      <td>-0.314868</td>\n",
       "      <td>-0.885046</td>\n",
       "      <td>-0.412587</td>\n",
       "      <td>-0.818729</td>\n",
       "      <td>-0.012022</td>\n",
       "      <td>...</td>\n",
       "      <td>1.030348</td>\n",
       "      <td>0.421886</td>\n",
       "      <td>-0.068029</td>\n",
       "      <td>0.258984</td>\n",
       "      <td>-0.057158</td>\n",
       "      <td>-0.834079</td>\n",
       "      <td>-0.441066</td>\n",
       "      <td>-0.099874</td>\n",
       "      <td>-0.829539</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.941448</td>\n",
       "      <td>0.165438</td>\n",
       "      <td>-0.392049</td>\n",
       "      <td>0.205172</td>\n",
       "      <td>3.134455</td>\n",
       "      <td>-0.161057</td>\n",
       "      <td>-0.982058</td>\n",
       "      <td>-0.499061</td>\n",
       "      <td>-0.884788</td>\n",
       "      <td>-0.341288</td>\n",
       "      <td>...</td>\n",
       "      <td>3.115495</td>\n",
       "      <td>2.232003</td>\n",
       "      <td>0.289214</td>\n",
       "      <td>6.179787</td>\n",
       "      <td>-0.052086</td>\n",
       "      <td>-0.932607</td>\n",
       "      <td>-0.753994</td>\n",
       "      <td>-0.415275</td>\n",
       "      <td>-0.885618</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.624388</td>\n",
       "      <td>-0.080687</td>\n",
       "      <td>-0.366623</td>\n",
       "      <td>-0.049865</td>\n",
       "      <td>1.192401</td>\n",
       "      <td>-0.295711</td>\n",
       "      <td>-0.877278</td>\n",
       "      <td>-0.429471</td>\n",
       "      <td>-0.818519</td>\n",
       "      <td>-0.060946</td>\n",
       "      <td>...</td>\n",
       "      <td>1.045350</td>\n",
       "      <td>0.420301</td>\n",
       "      <td>-0.071344</td>\n",
       "      <td>0.328387</td>\n",
       "      <td>-0.099864</td>\n",
       "      <td>-0.831247</td>\n",
       "      <td>-0.436137</td>\n",
       "      <td>-0.157848</td>\n",
       "      <td>-0.824938</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.360967</td>\n",
       "      <td>-0.055136</td>\n",
       "      <td>-0.222737</td>\n",
       "      <td>0.087353</td>\n",
       "      <td>0.521053</td>\n",
       "      <td>-0.297690</td>\n",
       "      <td>-0.790068</td>\n",
       "      <td>-0.485822</td>\n",
       "      <td>-0.745810</td>\n",
       "      <td>0.106832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249262</td>\n",
       "      <td>-0.195481</td>\n",
       "      <td>-0.132257</td>\n",
       "      <td>-0.800665</td>\n",
       "      <td>0.101134</td>\n",
       "      <td>-0.720496</td>\n",
       "      <td>-0.286264</td>\n",
       "      <td>0.140425</td>\n",
       "      <td>-0.767403</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.313211</td>\n",
       "      <td>-0.053253</td>\n",
       "      <td>-0.220183</td>\n",
       "      <td>0.105120</td>\n",
       "      <td>0.460798</td>\n",
       "      <td>-0.311198</td>\n",
       "      <td>-0.772821</td>\n",
       "      <td>-0.473726</td>\n",
       "      <td>-0.724301</td>\n",
       "      <td>0.113229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>-0.230969</td>\n",
       "      <td>-0.142840</td>\n",
       "      <td>-0.830226</td>\n",
       "      <td>0.126386</td>\n",
       "      <td>-0.694164</td>\n",
       "      <td>-0.241437</td>\n",
       "      <td>0.184116</td>\n",
       "      <td>-0.760070</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.353326</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>0.277299</td>\n",
       "      <td>-0.471843</td>\n",
       "      <td>-0.809794</td>\n",
       "      <td>0.672951</td>\n",
       "      <td>2.717927</td>\n",
       "      <td>0.928540</td>\n",
       "      <td>1.897364</td>\n",
       "      <td>-0.233773</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.782136</td>\n",
       "      <td>-0.502159</td>\n",
       "      <td>0.089402</td>\n",
       "      <td>-0.468237</td>\n",
       "      <td>-0.194680</td>\n",
       "      <td>2.013506</td>\n",
       "      <td>0.288960</td>\n",
       "      <td>-0.001121</td>\n",
       "      <td>1.840473</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feat_1    feat_2    feat_3    feat_4    feat_5    feat_6    feat_7  \\\n",
       "0 -0.900846  0.102587 -0.397814  0.112796  2.588096 -0.192754 -0.968311   \n",
       "1 -0.838868  0.039976 -0.387101  0.055413  2.066874 -0.226948 -0.947416   \n",
       "2 -0.814961 -0.010184 -0.397147  0.092713  1.897454 -0.269387 -0.945285   \n",
       "3 -0.110470  0.027849 -0.044310 -0.005343  0.177831 -0.232092 -0.562700   \n",
       "4 -0.626313 -0.091985 -0.373756 -0.005083  1.172486 -0.314868 -0.885046   \n",
       "5 -0.941448  0.165438 -0.392049  0.205172  3.134455 -0.161057 -0.982058   \n",
       "6 -0.624388 -0.080687 -0.366623 -0.049865  1.192401 -0.295711 -0.877278   \n",
       "7 -0.360967 -0.055136 -0.222737  0.087353  0.521053 -0.297690 -0.790068   \n",
       "8 -0.313211 -0.053253 -0.220183  0.105120  0.460798 -0.311198 -0.772821   \n",
       "9  1.353326 -0.034194  0.277299 -0.471843 -0.809794  0.672951  2.717927   \n",
       "\n",
       "     feat_8    feat_9   feat_10  ...   feat_24   feat_25   feat_26   feat_27  \\\n",
       "0 -0.490886 -0.872099 -0.288411  ...  2.541431  1.739102  0.166066  4.584869   \n",
       "1 -0.472817 -0.855387 -0.207101  ...  1.991721  1.259745  0.065058  3.019790   \n",
       "2 -0.449579 -0.849705 -0.151179  ...  1.822978  1.105511  0.065353  2.500681   \n",
       "3 -0.400713 -0.552356  0.037349  ... -0.098367 -0.370318 -0.123008 -0.861314   \n",
       "4 -0.412587 -0.818729 -0.012022  ...  1.030348  0.421886 -0.068029  0.258984   \n",
       "5 -0.499061 -0.884788 -0.341288  ...  3.115495  2.232003  0.289214  6.179787   \n",
       "6 -0.429471 -0.818519 -0.060946  ...  1.045350  0.420301 -0.071344  0.328387   \n",
       "7 -0.485822 -0.745810  0.106832  ...  0.249262 -0.195481 -0.132257 -0.800665   \n",
       "8 -0.473726 -0.724301  0.113229  ...  0.173077 -0.230969 -0.142840 -0.830226   \n",
       "9  0.928540  1.897364 -0.233773  ... -0.782136 -0.502159  0.089402 -0.468237   \n",
       "\n",
       "    feat_28   feat_29   feat_30   feat_31   feat_32  gt  \n",
       "0 -0.107031 -0.913990 -0.686416 -0.368085 -0.870545   0  \n",
       "1 -0.110633 -0.890023 -0.611625 -0.298235 -0.855208   0  \n",
       "2 -0.052730 -0.885691 -0.583346 -0.218140 -0.856456   0  \n",
       "3  0.106840 -0.483669 -0.224164  0.147321 -0.615051   0  \n",
       "4 -0.057158 -0.834079 -0.441066 -0.099874 -0.829539   0  \n",
       "5 -0.052086 -0.932607 -0.753994 -0.415275 -0.885618   0  \n",
       "6 -0.099864 -0.831247 -0.436137 -0.157848 -0.824938   0  \n",
       "7  0.101134 -0.720496 -0.286264  0.140425 -0.767403   0  \n",
       "8  0.126386 -0.694164 -0.241437  0.184116 -0.760070   0  \n",
       "9 -0.194680  2.013506  0.288960 -0.001121  1.840473   0  \n",
       "\n",
       "[10 rows x 33 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Total samples: 1000\n",
      " - Feature columns: 32 (excluding target 'gt')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preview\n",
    "print(\"First 10 entries in GENDER_CLASSIFICATION dataset:\")\n",
    "display(gender_df.head(10))\n",
    "\n",
    "# Dataset size\n",
    "print(f\"\\n - Total samples: {len(gender_df)}\")\n",
    "print(f\" - Feature columns: {gender_df.shape[1] - 1} (excluding target 'gt')\")\n",
    "\n",
    "# Gender distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "gender_df['gt'].value_counts().sort_index().plot(kind='bar', color=['#FF6F61', '#6B5B95'], edgecolor='grey')\n",
    "plt.title(\"Gender Distribution (0 = Female, 1 = Male)\", fontsize=14)\n",
    "plt.xlabel(\"Gender\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.xticks(ticks=[0, 1], labels=[\"Female\", \"Male\"], rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f94a4f",
   "metadata": {},
   "source": [
    "The same with the `ETHNICITY_CLASSIFICATION.csv`\n",
    "\n",
    "This dataset contains an equal number of samples for each of the five ethnicity classes (0 to 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fc7f67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " First 10 entries in ETHNICITY_CLASSIFICATION dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>feat_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_24</th>\n",
       "      <th>feat_25</th>\n",
       "      <th>feat_26</th>\n",
       "      <th>feat_27</th>\n",
       "      <th>feat_28</th>\n",
       "      <th>feat_29</th>\n",
       "      <th>feat_30</th>\n",
       "      <th>feat_31</th>\n",
       "      <th>feat_32</th>\n",
       "      <th>gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.502792</td>\n",
       "      <td>-0.349373</td>\n",
       "      <td>-0.068018</td>\n",
       "      <td>-0.627533</td>\n",
       "      <td>0.130331</td>\n",
       "      <td>0.373488</td>\n",
       "      <td>-0.491088</td>\n",
       "      <td>0.416753</td>\n",
       "      <td>-0.046255</td>\n",
       "      <td>0.325566</td>\n",
       "      <td>...</td>\n",
       "      <td>0.374539</td>\n",
       "      <td>-0.605601</td>\n",
       "      <td>0.579226</td>\n",
       "      <td>-0.119241</td>\n",
       "      <td>0.185425</td>\n",
       "      <td>-0.255194</td>\n",
       "      <td>0.705415</td>\n",
       "      <td>-0.027761</td>\n",
       "      <td>0.581276</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.741646</td>\n",
       "      <td>-0.240194</td>\n",
       "      <td>-0.006548</td>\n",
       "      <td>-0.639129</td>\n",
       "      <td>-0.059524</td>\n",
       "      <td>0.457087</td>\n",
       "      <td>-0.500733</td>\n",
       "      <td>0.345128</td>\n",
       "      <td>-0.040395</td>\n",
       "      <td>0.334946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353851</td>\n",
       "      <td>-0.651629</td>\n",
       "      <td>0.129014</td>\n",
       "      <td>-0.034723</td>\n",
       "      <td>0.392575</td>\n",
       "      <td>-0.289928</td>\n",
       "      <td>0.380891</td>\n",
       "      <td>-0.157109</td>\n",
       "      <td>0.742231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.836617</td>\n",
       "      <td>1.256781</td>\n",
       "      <td>2.227900</td>\n",
       "      <td>-0.603728</td>\n",
       "      <td>0.200403</td>\n",
       "      <td>1.366685</td>\n",
       "      <td>-0.666864</td>\n",
       "      <td>1.156750</td>\n",
       "      <td>-0.004516</td>\n",
       "      <td>0.767839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265453</td>\n",
       "      <td>-0.762246</td>\n",
       "      <td>-0.331476</td>\n",
       "      <td>0.051758</td>\n",
       "      <td>1.255034</td>\n",
       "      <td>-0.529064</td>\n",
       "      <td>1.412283</td>\n",
       "      <td>1.182029</td>\n",
       "      <td>3.046791</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.576996</td>\n",
       "      <td>1.784919</td>\n",
       "      <td>3.166102</td>\n",
       "      <td>-0.539903</td>\n",
       "      <td>0.108954</td>\n",
       "      <td>1.773622</td>\n",
       "      <td>-0.708488</td>\n",
       "      <td>2.679638</td>\n",
       "      <td>-0.313469</td>\n",
       "      <td>1.817676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.295006</td>\n",
       "      <td>-0.866968</td>\n",
       "      <td>2.203584</td>\n",
       "      <td>-0.428082</td>\n",
       "      <td>-0.368396</td>\n",
       "      <td>-0.398743</td>\n",
       "      <td>4.043780</td>\n",
       "      <td>2.281445</td>\n",
       "      <td>5.065996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.614504</td>\n",
       "      <td>1.103544</td>\n",
       "      <td>0.755605</td>\n",
       "      <td>-0.561434</td>\n",
       "      <td>-0.551708</td>\n",
       "      <td>0.936586</td>\n",
       "      <td>-0.575693</td>\n",
       "      <td>0.533321</td>\n",
       "      <td>0.033991</td>\n",
       "      <td>0.980126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155448</td>\n",
       "      <td>-0.825574</td>\n",
       "      <td>-0.627675</td>\n",
       "      <td>0.060466</td>\n",
       "      <td>0.815617</td>\n",
       "      <td>-0.434781</td>\n",
       "      <td>0.040881</td>\n",
       "      <td>-0.152861</td>\n",
       "      <td>2.191638</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.380426</td>\n",
       "      <td>0.704440</td>\n",
       "      <td>2.137562</td>\n",
       "      <td>-0.474417</td>\n",
       "      <td>0.526263</td>\n",
       "      <td>1.211204</td>\n",
       "      <td>-0.586933</td>\n",
       "      <td>1.593458</td>\n",
       "      <td>-0.309388</td>\n",
       "      <td>1.175341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.347852</td>\n",
       "      <td>-0.705405</td>\n",
       "      <td>2.125172</td>\n",
       "      <td>-0.377255</td>\n",
       "      <td>-0.388909</td>\n",
       "      <td>-0.391429</td>\n",
       "      <td>3.096098</td>\n",
       "      <td>1.798780</td>\n",
       "      <td>2.994499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.063048</td>\n",
       "      <td>0.793550</td>\n",
       "      <td>0.476156</td>\n",
       "      <td>-0.228052</td>\n",
       "      <td>-0.304069</td>\n",
       "      <td>0.463392</td>\n",
       "      <td>-0.484518</td>\n",
       "      <td>-0.123970</td>\n",
       "      <td>-0.026291</td>\n",
       "      <td>1.240902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193909</td>\n",
       "      <td>-0.694112</td>\n",
       "      <td>-0.053847</td>\n",
       "      <td>-0.142227</td>\n",
       "      <td>-0.351197</td>\n",
       "      <td>-0.415600</td>\n",
       "      <td>0.164695</td>\n",
       "      <td>-0.087266</td>\n",
       "      <td>1.136341</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.290171</td>\n",
       "      <td>1.062530</td>\n",
       "      <td>0.259989</td>\n",
       "      <td>0.101906</td>\n",
       "      <td>-0.357112</td>\n",
       "      <td>0.186110</td>\n",
       "      <td>-0.381772</td>\n",
       "      <td>-0.578399</td>\n",
       "      <td>0.071056</td>\n",
       "      <td>1.057730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042993</td>\n",
       "      <td>-0.569325</td>\n",
       "      <td>-0.626185</td>\n",
       "      <td>0.004535</td>\n",
       "      <td>-0.108778</td>\n",
       "      <td>-0.465900</td>\n",
       "      <td>-0.600877</td>\n",
       "      <td>-0.369860</td>\n",
       "      <td>0.422504</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.500149</td>\n",
       "      <td>1.309460</td>\n",
       "      <td>1.346507</td>\n",
       "      <td>-0.536384</td>\n",
       "      <td>-0.527906</td>\n",
       "      <td>1.199974</td>\n",
       "      <td>-0.596429</td>\n",
       "      <td>1.376919</td>\n",
       "      <td>-0.109738</td>\n",
       "      <td>1.608356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140938</td>\n",
       "      <td>-0.869237</td>\n",
       "      <td>0.443240</td>\n",
       "      <td>-0.152118</td>\n",
       "      <td>-0.048564</td>\n",
       "      <td>-0.391414</td>\n",
       "      <td>1.410651</td>\n",
       "      <td>0.505609</td>\n",
       "      <td>3.215973</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.080621</td>\n",
       "      <td>1.218115</td>\n",
       "      <td>1.569579</td>\n",
       "      <td>-0.633604</td>\n",
       "      <td>-0.435140</td>\n",
       "      <td>1.291967</td>\n",
       "      <td>-0.647991</td>\n",
       "      <td>1.772735</td>\n",
       "      <td>-0.120592</td>\n",
       "      <td>1.372363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202256</td>\n",
       "      <td>-0.874066</td>\n",
       "      <td>0.589836</td>\n",
       "      <td>-0.158117</td>\n",
       "      <td>0.258229</td>\n",
       "      <td>-0.346515</td>\n",
       "      <td>1.820684</td>\n",
       "      <td>0.771601</td>\n",
       "      <td>3.551059</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feat_1    feat_2    feat_3    feat_4    feat_5    feat_6    feat_7  \\\n",
       "0  0.502792 -0.349373 -0.068018 -0.627533  0.130331  0.373488 -0.491088   \n",
       "1  0.741646 -0.240194 -0.006548 -0.639129 -0.059524  0.457087 -0.500733   \n",
       "2  2.836617  1.256781  2.227900 -0.603728  0.200403  1.366685 -0.666864   \n",
       "3  2.576996  1.784919  3.166102 -0.539903  0.108954  1.773622 -0.708488   \n",
       "4  1.614504  1.103544  0.755605 -0.561434 -0.551708  0.936586 -0.575693   \n",
       "5  1.380426  0.704440  2.137562 -0.474417  0.526263  1.211204 -0.586933   \n",
       "6 -0.063048  0.793550  0.476156 -0.228052 -0.304069  0.463392 -0.484518   \n",
       "7 -0.290171  1.062530  0.259989  0.101906 -0.357112  0.186110 -0.381772   \n",
       "8  1.500149  1.309460  1.346507 -0.536384 -0.527906  1.199974 -0.596429   \n",
       "9  2.080621  1.218115  1.569579 -0.633604 -0.435140  1.291967 -0.647991   \n",
       "\n",
       "     feat_8    feat_9   feat_10  ...   feat_24   feat_25   feat_26   feat_27  \\\n",
       "0  0.416753 -0.046255  0.325566  ...  0.374539 -0.605601  0.579226 -0.119241   \n",
       "1  0.345128 -0.040395  0.334946  ...  0.353851 -0.651629  0.129014 -0.034723   \n",
       "2  1.156750 -0.004516  0.767839  ...  0.265453 -0.762246 -0.331476  0.051758   \n",
       "3  2.679638 -0.313469  1.817676  ...  0.295006 -0.866968  2.203584 -0.428082   \n",
       "4  0.533321  0.033991  0.980126  ...  0.155448 -0.825574 -0.627675  0.060466   \n",
       "5  1.593458 -0.309388  1.175341  ...  0.347852 -0.705405  2.125172 -0.377255   \n",
       "6 -0.123970 -0.026291  1.240902  ...  0.193909 -0.694112 -0.053847 -0.142227   \n",
       "7 -0.578399  0.071056  1.057730  ...  0.042993 -0.569325 -0.626185  0.004535   \n",
       "8  1.376919 -0.109738  1.608356  ...  0.140938 -0.869237  0.443240 -0.152118   \n",
       "9  1.772735 -0.120592  1.372363  ...  0.202256 -0.874066  0.589836 -0.158117   \n",
       "\n",
       "    feat_28   feat_29   feat_30   feat_31   feat_32  gt  \n",
       "0  0.185425 -0.255194  0.705415 -0.027761  0.581276   0  \n",
       "1  0.392575 -0.289928  0.380891 -0.157109  0.742231   0  \n",
       "2  1.255034 -0.529064  1.412283  1.182029  3.046791   0  \n",
       "3 -0.368396 -0.398743  4.043780  2.281445  5.065996   0  \n",
       "4  0.815617 -0.434781  0.040881 -0.152861  2.191638   0  \n",
       "5 -0.388909 -0.391429  3.096098  1.798780  2.994499   0  \n",
       "6 -0.351197 -0.415600  0.164695 -0.087266  1.136341   0  \n",
       "7 -0.108778 -0.465900 -0.600877 -0.369860  0.422504   0  \n",
       "8 -0.048564 -0.391414  1.410651  0.505609  3.215973   0  \n",
       "9  0.258229 -0.346515  1.820684  0.771601  3.551059   0  \n",
       "\n",
       "[10 rows x 33 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Total samples: 2500\n",
      " - Feature columns: 32 (excluding target 'gt')\n",
      " - Ethnicity class counts:\n",
      "Class 0: 500 samples\n",
      "Class 1: 500 samples\n",
      "Class 2: 500 samples\n",
      "Class 3: 500 samples\n",
      "Class 4: 500 samples\n"
     ]
    }
   ],
   "source": [
    "# Preview\n",
    "print(\"\\n First 10 entries in ETHNICITY_CLASSIFICATION dataset:\")\n",
    "display(ethnicity_df.head(10))\n",
    "\n",
    "# Dataset size\n",
    "print(f\"\\n - Total samples: {len(ethnicity_df)}\")\n",
    "print(f\" - Feature columns: {ethnicity_df.shape[1] - 1} (excluding target 'gt')\")\n",
    "\n",
    "# Ethnicity distribution\n",
    "ethnicity_counts = ethnicity_df['gt'].value_counts().sort_index()\n",
    "\n",
    "print(\" - Ethnicity class counts:\")\n",
    "for cls, count in ethnicity_counts.items():\n",
    "    print(f\"Class {cls}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56dc18f",
   "metadata": {},
   "source": [
    "## **Part 1: Multi-Layer-Perceptron for age regression**\n",
    "------\n",
    "In this part of the project, we aim to implement and train a **Multi-Layer Perceptron (MLP)** from scratch (without using deep learning libraries) to solve a **regression problem**: predicting a person's age based on deep features extracted from facial images in the UTKFace dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94931cdc",
   "metadata": {},
   "source": [
    "### **Objective**\n",
    "\n",
    "Given the dataset `AGE_PREDICTION.csv`, where each sample consists of a ResNet-extracted feature vector and an age label (`gt`), our goal is to:\n",
    "\n",
    "- Build an MLP with **at least 2 hidden layers**.\n",
    "- Minimize the **L2-regularized squared error loss**.\n",
    "- Train the model using an optimizer like `scipy.optimize` (without autograd).\n",
    "- Tune the hyperparameters via **k-fold cross-validation**.\n",
    "- Evaluate the model on train, validation, and test sets using:\n",
    "  - **Mean Squared Error (MSE)**\n",
    "  - **Mean Absolute Percentage Error (MAPE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e113528",
   "metadata": {},
   "source": [
    "### **Something about Theory**\n",
    "\n",
    "A Multi-Layer Perceptron is a feedforward neural network composed of layers of neurons. Each neuron performs a linear transformation followed by a non-linear activation. For this regression task, we define the loss function as:\n",
    "\n",
    "$$\n",
    "E(\\omega, \\beta) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{l=1}^{L} \\|\\omega^{(l)}\\|_2^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $y_i$ is the true age\n",
    "- $\\hat{y}_i$ is the predicted age\n",
    "- $\\omega^{(l)}$ is the weight matrix for layer $l$\n",
    "- $\\lambda$ is the regularization coefficient\n",
    "\n",
    "The **regularization term** helps prevent overfitting by penalizing large weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf0f6ee",
   "metadata": {},
   "source": [
    "**FIRST STEP**  \n",
    "1. **Data Preparation**:\n",
    "   - Load and inspect the dataset\n",
    "   - Standardize the input features\n",
    "   - Format inputs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b3a55e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âž¤ Separate features and target\n",
    "X = age_df.drop(columns=['gt']).values     # Convert directly to NumPy array\n",
    "y = age_df['gt'].values                    # Target as NumPy array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1a3ecc",
   "metadata": {},
   "source": [
    "**Data Splitting and Scaling Strategy**\n",
    "\n",
    "We reserve 20% of the dataset as a **hold-out test set** for final evaluation. The remaining 80% will be used for **k-fold cross-validation** to tune model hyperparameters.\n",
    "\n",
    "This ensures that the final test error reflects the generalization ability of the model after hyperparameter tuning.\n",
    "\n",
    "\n",
    "We must always fit the scaler (`fit_transform`) only on the training set, and apply the transformation (`transform`) to the test set.\n",
    "\n",
    "This avoids using test set statistics (mean, std) during training â€” which would lead to overfitting and invalid results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c06fdc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the training set: 16380\n",
      "Number of samples in the test set: 4095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split the data\n",
    "X_tr, X_ts, Y_tr, Y_ts = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Now fit the scaler ONLY on the trainval set\n",
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(X_tr)\n",
    "X_ts = scaler.transform(X_ts)  # Use the same mean/std from training\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "print(f'Number of samples in the training set: {X_tr.shape[0]}')\n",
    "print(f'Number of samples in the test set: {X_ts.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cb5e35",
   "metadata": {},
   "source": [
    "**Hyperparameter Configuration for Cross-Validation**\n",
    "\n",
    "To identify the best model architecture and regularization strength, we define a set of candidate configurations.\n",
    "\n",
    "Each configuration specifies:\n",
    "- The number of layers and neurons per layer\n",
    "- The activation function (`'tanh'` or `'sigmoid'`)\n",
    "- The L2 regularization parameter Î»\n",
    "\n",
    "These configurations will be evaluated using **k-fold cross-validation**, and the one with the lowest average validation error (MAPE or MSE) will be selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6683594",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_tr.shape[1]  # Number of input features\n",
    "\n",
    "configs = [\n",
    "    # 1 hidden layer, light regularization\n",
    "    {\"layers\": [input_dim, 32, 1], \"activation\": \"tanh\", \"lambda\": 1e-4},\n",
    "    \n",
    "    # 2 hidden layers, moderate regularization\n",
    "    {\"layers\": [input_dim, 64, 32, 1], \"activation\": \"tanh\", \"lambda\": 1e-3},\n",
    "    \n",
    "    # 3 hidden layers, sigmoid activation\n",
    "    {\"layers\": [input_dim, 64, 64, 32, 1], \"activation\": \"sigmoid\", \"lambda\": 1e-4},\n",
    "    \n",
    "    # 2 hidden layers, larger network and stronger regularization\n",
    "    {\"layers\": [input_dim, 128, 64, 1], \"activation\": \"tanh\", \"lambda\": 1e-2},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4508c0",
   "metadata": {},
   "source": [
    "**Performing 5-Fold Cross-Validation**\n",
    "\n",
    "We now evaluate each configuration using **5-fold cross-validation** on the training set.\n",
    "\n",
    "For each configuration:\n",
    "- The model is trained on 4 folds and validated on the remaining fold.\n",
    "- This process repeats 5 times, each time using a different fold as the validation set.\n",
    "- The validation performance is averaged across folds using **Mean Absolute Percentage Error (MAPE)** as the metric.\n",
    "\n",
    "The configuration with the **lowest average MAPE** will be selected for final training and evaluation on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80ab0710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold cross-validation with 4 configurations...\n",
      "\n",
      "Testing config 1/4: {'layers': [32, 32, 1], 'activation': 'tanh', 'lambda': 0.0001}\n",
      "  Fold 1: MAPE = 96.4569, Train time = 0.29s, Converged = False\n",
      "  Fold 2: MAPE = 96.4567, Train time = 0.24s, Converged = False\n",
      "  Fold 3: MAPE = 95.4432, Train time = 0.27s, Converged = False\n",
      "  Fold 4: MAPE = 96.4479, Train time = 0.25s, Converged = False\n",
      "  Fold 5: MAPE = 95.9715, Train time = 0.24s, Converged = False\n",
      "  Average MAPE: 96.1552 Â± 0.4021, Avg time per fold: 0.26s\n",
      "\n",
      "Testing config 2/4: {'layers': [32, 64, 32, 1], 'activation': 'tanh', 'lambda': 0.001}\n",
      "  Fold 1: MAPE = 110.2185, Train time = 1.82s, Converged = False\n",
      "  Fold 2: MAPE = 112.5835, Train time = 1.81s, Converged = False\n",
      "  Fold 3: MAPE = 111.7996, Train time = 2.01s, Converged = False\n",
      "  Fold 4: MAPE = 113.1717, Train time = 2.08s, Converged = False\n",
      "  Fold 5: MAPE = 111.9268, Train time = 2.03s, Converged = False\n",
      "  Average MAPE: 111.9400 Â± 0.9914, Avg time per fold: 1.96s\n",
      "\n",
      "Testing config 3/4: {'layers': [32, 64, 64, 32, 1], 'activation': 'sigmoid', 'lambda': 0.0001}\n",
      "  Fold 1: MAPE = 26.0797, Train time = 8.75s, Converged = False\n",
      "  Fold 2: MAPE = 26.2230, Train time = 5.19s, Converged = False\n",
      "  Fold 3: MAPE = 28.5932, Train time = 9.44s, Converged = False\n",
      "  Fold 4: MAPE = 27.4561, Train time = 5.18s, Converged = False\n",
      "  Fold 5: MAPE = 26.2938, Train time = 7.32s, Converged = False\n",
      "  Average MAPE: 26.9291 Â± 0.9665, Avg time per fold: 7.19s\n",
      "\n",
      "Testing config 4/4: {'layers': [32, 128, 64, 1], 'activation': 'tanh', 'lambda': 0.01}\n",
      "  Fold 1: MAPE = 67.5559, Train time = 3.34s, Converged = False\n",
      "  Fold 2: MAPE = 68.9262, Train time = 3.48s, Converged = False\n",
      "  Fold 3: MAPE = 70.1810, Train time = 3.37s, Converged = False\n",
      "  Fold 4: MAPE = 68.6547, Train time = 3.37s, Converged = False\n",
      "  Fold 5: MAPE = 68.6787, Train time = 3.26s, Converged = False\n",
      "  Average MAPE: 68.7993 Â± 0.8374, Avg time per fold: 3.38s\n",
      "\n",
      "Best configuration: {'layers': [32, 64, 64, 32, 1], 'activation': 'sigmoid', 'lambda': 0.0001}\n",
      "Best MAPE: 26.9291 Â± 0.9665\n",
      "\n",
      " - Best Config Selected: {'layers': [32, 64, 64, 32, 1], 'activation': 'sigmoid', 'lambda': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "from src.Functions_1j_Pinos_Lattanzi import cross_validate_model\n",
    "\n",
    "# Run k-fold CV and select the configuration with lowest avg MAPE\n",
    "best_config, all_results = cross_validate_model(X_tr, Y_tr, k_folds=5, configs=configs, scoring='mape')\n",
    "\n",
    "print(\"\\n - Best Config Selected:\", best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2be5c0",
   "metadata": {},
   "source": [
    "**Final Model Training on Full Training Set**\n",
    "\n",
    "Using the best configuration selected via cross-validation, we now train the final MLP model on the **entire training set**.\n",
    "\n",
    "This training is performed using the `scipy.optimize.minimize` function with the **L-BFGS-B optimizer**, which minimizes the L2-regularized MSE loss.\n",
    "\n",
    "The optimization process returns:\n",
    "- The trained model with optimal weights\n",
    "- The optimizer result object (containing iterations, status, etc.)\n",
    "- The total optimization time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed2179c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Functions_1j_Pinos_Lattanzi import train_model\n",
    "\n",
    "y_min = Y_tr.min()\n",
    "y_max = Y_tr.max()\n",
    "Y_tr_norm = (Y_tr - y_min) / (y_max - y_min)\n",
    "\n",
    "# âž¤ Train final model on normalized targets\n",
    "final_model, opt_result, train_time = train_model(\n",
    "    X_tr, Y_tr_norm,\n",
    "    layer_sizes=best_config['layers'],\n",
    "    activation=best_config['activation'],\n",
    "    lambda_reg=best_config['lambda']\n",
    ")\n",
    "\n",
    "# âž¤ Predict and denormalize predictions\n",
    "y_tr_pred = final_model.predict(X_tr) * (y_max - y_min) + y_min\n",
    "y_ts_pred = final_model.predict(X_ts) * (y_max - y_min) + y_min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5198b4c",
   "metadata": {},
   "source": [
    "**Model Evaluation and Final Results**\n",
    "\n",
    "After training the final MLP on the full training set, we evaluate its performance on both:\n",
    "- The training set (to measure fit)\n",
    "- The test set (to assess generalization)\n",
    "\n",
    "We also print a summary of:\n",
    "- The selected architecture and hyperparameters\n",
    "- Performance metrics (MSE, MAPE)\n",
    "- Optimizer details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10a18802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FINAL MODEL PERFORMANCE ---\n",
      "Training MSE  : 128.3271\n",
      "Training MAPE : 26.84%\n",
      "Test MSE      : 130.2091\n",
      "Test MAPE     : 27.72%\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Evaluate and print performance summary\n",
    "\n",
    "# Compute performance metrics\n",
    "mse_train = mean_squared_error(Y_tr, y_tr_pred)\n",
    "mape_train = mean_absolute_percentage_error(Y_tr, y_tr_pred) * 100\n",
    "\n",
    "mse_test = mean_squared_error(Y_ts, y_ts_pred)\n",
    "mape_test = mean_absolute_percentage_error(Y_ts, y_ts_pred) * 100\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n--- FINAL MODEL PERFORMANCE ---\")\n",
    "print(f\"Training MSE  : {mse_train:.4f}\")\n",
    "print(f\"Training MAPE : {mape_train:.2f}%\")\n",
    "print(f\"Test MSE      : {mse_test:.4f}\")\n",
    "print(f\"Test MAPE     : {mape_test:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b42c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the training set: 16380\n",
      "Number of samples in the test set: 4095\n",
      "Starting 5-fold cross-validation with 6 configurations...\n",
      "\n",
      "Testing config 1/6: {'layers': [32, 32, 1], 'activation': 'tanh', 'lambda': 0.0001}\n",
      "  Fold 1: MAPE = 96.4569, Train time = 0.26s, Converged = False\n",
      "  Fold 2: MAPE = 96.4567, Train time = 0.23s, Converged = False\n",
      "  Fold 3: MAPE = 95.4432, Train time = 0.22s, Converged = False\n",
      "  Fold 4: MAPE = 96.4479, Train time = 0.20s, Converged = False\n",
      "  Fold 5: MAPE = 95.9715, Train time = 0.22s, Converged = False\n",
      "  Average MAPE: 96.1552 Â± 0.4021, Avg time per fold: 0.23s\n",
      "\n",
      "Testing config 2/6: {'layers': [32, 64, 32, 1], 'activation': 'tanh', 'lambda': 0.001}\n",
      "  Fold 1: MAPE = 110.2185, Train time = 1.78s, Converged = False\n",
      "  Fold 2: MAPE = 112.5835, Train time = 1.92s, Converged = False\n",
      "  Fold 3: MAPE = 111.7996, Train time = 1.70s, Converged = False\n",
      "  Fold 4: MAPE = 113.1717, Train time = 1.86s, Converged = False\n",
      "  Fold 5: MAPE = 111.9268, Train time = 8.48s, Converged = False\n",
      "  Average MAPE: 111.9400 Â± 0.9914, Avg time per fold: 3.15s\n",
      "\n",
      "Testing config 3/6: {'layers': [32, 64, 64, 32, 1], 'activation': 'sigmoid', 'lambda': 0.0001}\n",
      "  Fold 1: MAPE = 26.0797, Train time = 8.95s, Converged = False\n",
      "  Fold 2: MAPE = 26.2230, Train time = 5.30s, Converged = False\n",
      "  Fold 3: MAPE = 28.5932, Train time = 9.53s, Converged = False\n",
      "  Fold 4: MAPE = 27.4561, Train time = 5.15s, Converged = False\n",
      "  Fold 5: MAPE = 26.2938, Train time = 7.32s, Converged = False\n",
      "  Average MAPE: 26.9291 Â± 0.9665, Avg time per fold: 7.26s\n",
      "\n",
      "Testing config 4/6: {'layers': [32, 128, 64, 1], 'activation': 'tanh', 'lambda': 0.01}\n",
      "  Fold 1: MAPE = 67.5559, Train time = 3.33s, Converged = False\n",
      "  Fold 2: MAPE = 68.9262, Train time = 3.28s, Converged = False\n",
      "  Fold 3: MAPE = 70.1810, Train time = 3.26s, Converged = False\n",
      "  Fold 4: MAPE = 68.6547, Train time = 3.27s, Converged = False\n",
      "  Fold 5: MAPE = 68.6787, Train time = 3.23s, Converged = False\n",
      "  Average MAPE: 68.7993 Â± 0.8374, Avg time per fold: 3.29s\n",
      "\n",
      "Testing config 5/6: {'layers': [32, 64, 32, 1], 'activation': 'relu', 'lambda': 0.0001}\n",
      "  Fold 1: MAPE = 414.1112, Train time = 0.95s, Converged = False\n",
      "  Fold 2: MAPE = 426.9532, Train time = 0.93s, Converged = False\n",
      "  Fold 3: MAPE = 437.0929, Train time = 0.91s, Converged = False\n",
      "  Fold 4: MAPE = 433.5546, Train time = 0.98s, Converged = False\n",
      "  Fold 5: MAPE = 427.0790, Train time = 0.96s, Converged = False\n",
      "  Average MAPE: 427.7582 Â± 7.8496, Avg time per fold: 0.95s\n",
      "\n",
      "Testing config 6/6: {'layers': [32, 128, 64, 1], 'activation': 'relu', 'lambda': 0.001}\n",
      "  Fold 1: MAPE = 160.1549, Train time = 4.24s, Converged = False\n",
      "  Fold 2: MAPE = 159.8536, Train time = 2.44s, Converged = False\n",
      "  Fold 3: MAPE = 163.1193, Train time = 2.43s, Converged = False\n",
      "  Fold 4: MAPE = 160.6705, Train time = 2.44s, Converged = False\n",
      "  Fold 5: MAPE = 162.4828, Train time = 2.44s, Converged = False\n",
      "  Average MAPE: 161.2562 Â± 1.3037, Avg time per fold: 2.80s\n",
      "\n",
      "Best configuration: {'layers': [32, 64, 64, 32, 1], 'activation': 'sigmoid', 'lambda': 0.0001}\n",
      "Best MAPE: 26.9291 Â± 0.9665\n",
      "\n",
      " - Best Config Selected: {'layers': [32, 64, 64, 32, 1], 'activation': 'sigmoid', 'lambda': 0.0001}\n",
      "\n",
      "Training completed in 10.08 seconds\n",
      "Optimizer converged: False\n",
      "Final loss: 0.020568\n",
      "Number of iterations: 8\n",
      "\n",
      "==================================================\n",
      "         FINAL MODEL PERFORMANCE\n",
      "==================================================\n",
      "Training Set:\n",
      "  MSE   : 128.3271\n",
      "  MAE   : 8.4493\n",
      "  MAPE  : 26.84%\n",
      "  RÂ²    : 0.5355\n",
      "\n",
      "Test Set:\n",
      "  MSE   : 130.2091\n",
      "  MAE   : 8.5168\n",
      "  MAPE  : 27.72%\n",
      "  RÂ²    : 0.5246\n",
      "\n",
      "Overfitting Analysis:\n",
      "  MSE Ratio (Test/Train): 1.015\n",
      "  MAPE Difference: 0.88%\n",
      "  âœ… Good generalization!\n",
      "\n",
      "Prediction Error Analysis:\n",
      "Training errors - Mean: 0.098, Std: 11.328\n",
      "Test errors     - Mean: -0.327, Std: 11.406\n",
      "\n",
      "Age Range Analysis:\n",
      "Training age range: 10.0 - 89.0 years\n",
      "Test age range: 10.0 - 89.0 years\n",
      "Training pred range: 21.1 - 59.9 years\n",
      "Test pred range: 21.4 - 59.7 years\n"
     ]
    }
   ],
   "source": [
    "# Your existing data loading and splitting code is good!\n",
    "# âž¤ Separate features and target\n",
    "X = age_df.drop(columns=['gt']).values     # Convert directly to NumPy array\n",
    "y = age_df['gt'].values                    # Target as NumPy array\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split the data\n",
    "X_tr, X_ts, Y_tr, Y_ts = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Now fit the scaler ONLY on the trainval set\n",
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(X_tr)\n",
    "X_ts = scaler.transform(X_ts)  # Use the same mean/std from training\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "print(f'Number of samples in the training set: {X_tr.shape[0]}')\n",
    "print(f'Number of samples in the test set: {X_ts.shape[0]}')\n",
    "\n",
    "input_dim = X_tr.shape[1]  # Number of input features\n",
    "\n",
    "# ============================================================================\n",
    "# IMPROVED CONFIGURATIONS - More comprehensive and modern\n",
    "# ============================================================================\n",
    "\n",
    "configs = [\n",
    "    # Original configurations (for comparison)\n",
    "    {\"layers\": [input_dim, 32, 1], \"activation\": \"tanh\", \"lambda\": 1e-4},\n",
    "    {\"layers\": [input_dim, 64, 32, 1], \"activation\": \"tanh\", \"lambda\": 1e-3},\n",
    "    {\"layers\": [input_dim, 64, 64, 32, 1], \"activation\": \"sigmoid\", \"lambda\": 1e-4},\n",
    "    {\"layers\": [input_dim, 128, 64, 1], \"activation\": \"tanh\", \"lambda\": 1e-2},\n",
    "    \n",
    "    # NEW: ReLU-based configurations (often work better for regression)\n",
    "    {\"layers\": [input_dim, 64, 32, 1], \"activation\": \"relu\", \"lambda\": 1e-4},\n",
    "    {\"layers\": [input_dim, 128, 64, 1], \"activation\": \"relu\", \"lambda\": 1e-3},\n",
    "]\n",
    "\n",
    "# Run cross-validation with more iterations for better convergence\n",
    "from src.function2 import cross_validate_model\n",
    "\n",
    "best_config, all_results = cross_validate_model(\n",
    "    X_tr, Y_tr, \n",
    "    k_folds=5, \n",
    "    configs=configs, \n",
    "    max_iter=500,  # Increased iterations\n",
    "    scoring='mape'\n",
    ")\n",
    "\n",
    "print(\"\\n - Best Config Selected:\", best_config)\n",
    "\n",
    "# ============================================================================\n",
    "# IMPROVED TRAINING WITH BETTER SETTINGS\n",
    "# ============================================================================\n",
    "\n",
    "from src.Functions_1j_Pinos_Lattanzi import train_model\n",
    "\n",
    "# Target normalization (same as before)\n",
    "y_min = Y_tr.min()\n",
    "y_max = Y_tr.max()\n",
    "Y_tr_norm = (Y_tr - y_min) / (y_max - y_min)\n",
    "\n",
    "# âž¤ Train final model with MORE iterations for final training\n",
    "final_model, opt_result, train_time = train_model(\n",
    "    X_tr, Y_tr_norm,\n",
    "    layer_sizes=best_config['layers'],\n",
    "    activation=best_config['activation'],\n",
    "    lambda_reg=best_config['lambda'],\n",
    "    dropout_rate=best_config.get('dropout', 0.0),  # Handle dropout if present\n",
    "    max_iter=1000,  # More iterations for final model\n",
    "    method='L-BFGS-B'  # You can also try 'BFGS' or 'CG'\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining completed in {train_time:.2f} seconds\")\n",
    "print(f\"Optimizer converged: {opt_result.success}\")\n",
    "print(f\"Final loss: {opt_result.fun:.6f}\")\n",
    "print(f\"Number of iterations: {opt_result.nit}\")\n",
    "\n",
    "# âž¤ Predict and denormalize predictions\n",
    "y_tr_pred = final_model.predict(X_tr) * (y_max - y_min) + y_min\n",
    "y_ts_pred = final_model.predict(X_ts) * (y_max - y_min) + y_min\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION WITH ADDITIONAL METRICS\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"Calculate MAPE with zero handling\"\"\"\n",
    "    mask = np.abs(y_true) > 0.1  # Avoid division by very small numbers\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.inf\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))\n",
    "\n",
    "# Compute comprehensive performance metrics\n",
    "mse_train = mean_squared_error(Y_tr, y_tr_pred)\n",
    "mae_train = mean_absolute_error(Y_tr, y_tr_pred)\n",
    "mape_train = mean_absolute_percentage_error(Y_tr, y_tr_pred) * 100\n",
    "\n",
    "mse_test = mean_squared_error(Y_ts, y_ts_pred)\n",
    "mae_test = mean_absolute_error(Y_ts, y_ts_pred)\n",
    "mape_test = mean_absolute_percentage_error(Y_ts, y_ts_pred) * 100\n",
    "\n",
    "# Calculate RÂ² score\n",
    "def r2_score(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "r2_train = r2_score(Y_tr, y_tr_pred)\n",
    "r2_test = r2_score(Y_ts, y_ts_pred)\n",
    "\n",
    "# Print comprehensive summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"         FINAL MODEL PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training Set:\")\n",
    "print(f\"  MSE   : {mse_train:.4f}\")\n",
    "print(f\"  MAE   : {mae_train:.4f}\")\n",
    "print(f\"  MAPE  : {mape_train:.2f}%\")\n",
    "print(f\"  RÂ²    : {r2_train:.4f}\")\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  MSE   : {mse_test:.4f}\")\n",
    "print(f\"  MAE   : {mae_test:.4f}\")\n",
    "print(f\"  MAPE  : {mape_test:.2f}%\")\n",
    "print(f\"  RÂ²    : {r2_test:.4f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "print(f\"\\nOverfitting Analysis:\")\n",
    "print(f\"  MSE Ratio (Test/Train): {mse_test/mse_train:.3f}\")\n",
    "print(f\"  MAPE Difference: {mape_test - mape_train:.2f}%\")\n",
    "\n",
    "if mse_test/mse_train > 1.5:\n",
    "    print(\"  âš ï¸  Possible overfitting detected!\")\n",
    "elif mse_test/mse_train < 0.9:\n",
    "    print(\"  â„¹ï¸  Model might be underfitting\")\n",
    "else:\n",
    "    print(\"  âœ… Good generalization!\")\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: PREDICTION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "# Analyze prediction errors\n",
    "train_errors = Y_tr - y_tr_pred\n",
    "test_errors = Y_ts - y_ts_pred\n",
    "\n",
    "print(f\"\\nPrediction Error Analysis:\")\n",
    "print(f\"Training errors - Mean: {np.mean(train_errors):.3f}, Std: {np.std(train_errors):.3f}\")\n",
    "print(f\"Test errors     - Mean: {np.mean(test_errors):.3f}, Std: {np.std(test_errors):.3f}\")\n",
    "\n",
    "# Age range analysis\n",
    "print(f\"\\nAge Range Analysis:\")\n",
    "print(f\"Training age range: {Y_tr.min():.1f} - {Y_tr.max():.1f} years\")\n",
    "print(f\"Test age range: {Y_ts.min():.1f} - {Y_ts.max():.1f} years\")\n",
    "print(f\"Training pred range: {y_tr_pred.min():.1f} - {y_tr_pred.max():.1f} years\")\n",
    "print(f\"Test pred range: {y_ts_pred.min():.1f} - {y_ts_pred.max():.1f} years\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
