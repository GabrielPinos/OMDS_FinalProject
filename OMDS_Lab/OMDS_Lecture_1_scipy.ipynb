{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Methods for Data Science\n",
    "### A.A. 2024-2025\n",
    "\n",
    "Alessandro Pannone\n",
    "Ph.D. Student @ Sapienza University of Rome\n",
    "\n",
    "pannone@diag.uniroma1.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWMt6ZpYKuCb"
   },
   "source": [
    "## Matplotlib\n",
    "Documentation: https://matplotlib.org/\n",
    "\n",
    "Matplotlib is a Python library for 2D and 3D plotting in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 525,
     "status": "ok",
     "timestamp": 1679492323562,
     "user": {
      "displayName": "Cecilia Salvatore",
      "userId": "06475737550872053301"
     },
     "user_tz": -60
    },
    "id": "VPGci7UPLZzb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 1938,
     "status": "ok",
     "timestamp": 1679486371314,
     "user": {
      "displayName": "Cecilia Salvatore",
      "userId": "06475737550872053301"
     },
     "user_tz": -60
    },
    "id": "iHzJEsJQL_6H",
    "outputId": "b000aff8-f8e1-4f21-c13b-7c4ad3fcb4f5"
   },
   "outputs": [],
   "source": [
    "# DRAW SIMPLE PLOT\n",
    "x = np.linspace(0, 2, 100)\n",
    "y = np.random.random(100)\n",
    "\n",
    "plt.plot(x,y,'g-')\n",
    "plt.xlabel('x label')\n",
    "plt.ylabel('y label')\n",
    "plt.title(\"Simple Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3fDei4mNLZO"
   },
   "source": [
    "And what if I want to compare different __2D functions__ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 1083,
     "status": "ok",
     "timestamp": 1679486676381,
     "user": {
      "displayName": "Cecilia Salvatore",
      "userId": "06475737550872053301"
     },
     "user_tz": -60
    },
    "id": "L8iVhObpMIkh",
    "outputId": "6d483e2c-7903-44a5-b9de-4ec8465df902"
   },
   "outputs": [],
   "source": [
    "plt.plot(x, x, color='red')\n",
    "plt.plot(x, x**2, color='blue')\n",
    "plt.plot(x, 2**x, color='green')\n",
    "\n",
    "plt.xlabel('x label')\n",
    "plt.ylabel('y label')\n",
    "\n",
    "plt.title(\"Comparing functions\")\n",
    "\n",
    "plt.legend(['Linear','Quadratic','Exponential'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jO-LfMycNS6p"
   },
   "source": [
    "And what about __plotting a 3D function__?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1679492623622,
     "user": {
      "displayName": "Cecilia Salvatore",
      "userId": "06475737550872053301"
     },
     "user_tz": -60
    },
    "id": "hKSODUa3NTOj"
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def plotting(funct: Callable, title: str):\n",
    "    #create the object empty figure\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    #create the grid\n",
    "    x = np.linspace(-5, 5, 50) #create 50 points between [-5,5] evenly spaced\n",
    "    y = np.linspace(-5, 5, 50)\n",
    "    X, Y = np.meshgrid(x, y) #create the grid for the plot\n",
    "\n",
    "    Z = funct(X, Y) #evaluate the function (note that X,Y,Z are matrix)\n",
    "\n",
    "\n",
    "    ax.plot_surface(X, Y, Z, rstride=1, cstride=1,cmap='viridis', edgecolor='none')\n",
    "\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('z')\n",
    "    ax.set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 271
    },
    "executionInfo": {
     "elapsed": 803,
     "status": "ok",
     "timestamp": 1679492628043,
     "user": {
      "displayName": "Cecilia Salvatore",
      "userId": "06475737550872053301"
     },
     "user_tz": -60
    },
    "id": "RArx_rSmNbeu",
    "outputId": "4dee8d27-8ba7-45b3-ce6f-e61dbb907c18"
   },
   "outputs": [],
   "source": [
    "def paraboloid(x,y):\n",
    "    return y**2 + x**2\n",
    "\n",
    "plotting(paraboloid,'A simple paraboloid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buB2OnglNhGn"
   },
   "source": [
    "## Unconstrained Optimization in Python\n",
    "\n",
    "Suppose you have a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ you want to optimize, i.e. you are solving:\n",
    "$$ \\min_{x \\in \\mathbb{R}^n} f(x)\n",
    "$$\n",
    "\n",
    "If you are in a supervised learning contest, $f(x)$ could be your error function depending on the parameters of your neural network.\n",
    "\n",
    "Let's think your objective function is:\n",
    "$$ f(x) = 4x_{1}^2+10x_{2}^2+12x_{1}^2 x_{2}^2\n",
    "$$\n",
    "where $x = (x_1,x_2) \\in \\mathbb{R}^2$\n",
    "\n",
    "We firstly write the function and its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE FUNCTION TO OPTIMIZE\n",
    "def f(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    NOTE: If you want to optimize a function, it should take only ONE VARIABLE argument --> put them inside an array!\n",
    "    NOTE: Variable argument must be the FIRST ARGUMENT of the function! (then, you can add as many arguments you want)\n",
    "    \"\"\"\n",
    "\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "\n",
    "    return 4*x1**2 + 10*x2**2 + 12*(x1**2)*(x2**2)\n",
    "\n",
    "def f_for_plotting(x1: float, x2: float) -> float:\n",
    "    \"\"\"\n",
    "    Same function defined above, used only for plotting\n",
    "    \"\"\"\n",
    "\n",
    "    return 4*x1**2 + 10*x2**2 + 12*(x1**2)*(x2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 271
    },
    "executionInfo": {
     "elapsed": 1558,
     "status": "ok",
     "timestamp": 1679497570620,
     "user": {
      "displayName": "Cecilia Salvatore",
      "userId": "06475737550872053301"
     },
     "user_tz": -60
    },
    "id": "qdFmLqDkkAqR",
    "outputId": "f1bd1f1b-d564-400e-f110-974d31d9d3cd"
   },
   "outputs": [],
   "source": [
    "plotting(f_for_plotting,'The objective function in [-5,5]x[-5,5]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRDKToB5khbq"
   },
   "source": [
    "__Implement your own method__\n",
    "\n",
    "If we do not know any optimization library, we have to implement our own algorithm to solve this optimization problem.\n",
    "\n",
    "We are tackling a quite trivial problem. Let's also use the most basic unconstrained optimization method, the batch __gradient descent method__ with a fixed step-size $\\eta$\n",
    "\n",
    "The update rule is very simple. We fix $x_{k+1} = x_k - \\eta \\nabla f(x_k)$ until we get $$ \\vert \\vert \\nabla f(x_k) \\vert \\vert \\le \\varepsilon $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE NEED TO IMPLEMENT THE GRADIENT FIRST\n",
    "def df_dx(x: np.ndarray) -> np.array:\n",
    "    \"\"\"\n",
    "    Computes gradient of function w.r.t. x variables\n",
    "    NOTE: Return an array of gradients that has the same shape as the provided x!\n",
    "    \"\"\"\n",
    "\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "\n",
    "    df_dx1 = 8*x1 + 24*(x2**2)*x1\n",
    "    df_dx2 = 20*x2 + 24*(x1**2)*x2\n",
    "\n",
    "    return np.array([df_dx1, df_dx2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, Any\n",
    "\n",
    "def gradient_descent(\n",
    "        f: Callable[[np.ndarray], np.ndarray],\n",
    "        grad: Callable[[np.ndarray], np.ndarray],\n",
    "        x_0: np.ndarray,\n",
    "        eta: float,\n",
    "        max_it: int,\n",
    "        tol: float\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Gradient descent algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    f: function to to optimize\n",
    "    grad: gradient of f\n",
    "    x_0: starting point\n",
    "    eta: step size,\n",
    "    max_it: max iterations\n",
    "    tol: tolerance (if norm of gradient < tol --> stationary point found)\n",
    "\n",
    "    Returns:\n",
    "    Dict {\n",
    "        f_star: np.ndarray (best value)\n",
    "        x_star: np.ndarray (best variables)\n",
    "        grad_norm: np.ndarray (norm of gradient evaluated in x*)\n",
    "        success: bool (True if stationary point found)\n",
    "        iterations: int (number of iterations done)\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    # INITIALIZATION\n",
    "    x = x_0\n",
    "    k = 0\n",
    "    success = False\n",
    "    \n",
    "    for k in range(max_it):\n",
    "        # COMPUTE GRADIENT\n",
    "        df_dx = grad(x)\n",
    "\n",
    "        # CHECK IF CURRENT POINT IS STATIONARY\n",
    "        norm = np.linalg.norm(df_dx)\n",
    "        if norm < tol:\n",
    "            success = True\n",
    "            break\n",
    "\n",
    "        # CHECK IF GRADIENT EXPLODED\n",
    "        if norm > 1e10:\n",
    "            break\n",
    "\n",
    "        # UPDATE CURRENT POINT\n",
    "        x = x - eta * df_dx\n",
    "\n",
    "    return {\n",
    "        \"f_star\": f(x),\n",
    "        \"x_star\": x,\n",
    "        \"grad_norm\": grad(x),\n",
    "        \"success\": success,\n",
    "        \"iterations\": k\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 441,
     "status": "ok",
     "timestamp": 1679497575493,
     "user": {
      "displayName": "Cecilia Salvatore",
      "userId": "06475737550872053301"
     },
     "user_tz": -60
    },
    "id": "LwAmYfJsvVz9",
    "outputId": "e86c834b-be17-4d23-e1c8-7642d54cc2d6"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "x_0 = np.array([-2,2])\n",
    "\n",
    "start = time.time()\n",
    "result = gradient_descent(f=f, grad=df_dx, x_0=x_0, eta=1e-4, max_it=100000, tol=1e-6)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Result:\")\n",
    "pprint(result)\n",
    "\n",
    "print(\"Duration: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlKQpnWBwuy9"
   },
   "source": [
    "And what if we do not want write down the gradient?\n",
    "\n",
    "You can use __authomatic differentiation__ with __autograd__ library... but you will pay a __high price__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20326,
     "status": "ok",
     "timestamp": 1679496025750,
     "user": {
      "displayName": "Cecilia Salvatore",
      "userId": "06475737550872053301"
     },
     "user_tz": -60
    },
    "id": "ZpdeC6RvwsTa",
    "outputId": "e63f8eff-dad3-49bc-e61e-3a2d3c483b65"
   },
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "\n",
    "x_0 = np.array([-0.5,0.5])\n",
    "\n",
    "start = time.time()\n",
    "result =  gradient_descent(f=f, grad=grad(f), x_0=x_0, eta=1e-4, max_it=100000, tol=1e-6)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Result:\")\n",
    "pprint(result)\n",
    "\n",
    "print(\"Duration: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZx0E7YlxMdb"
   },
   "source": [
    "__Almost 100 TIMES SLOWER!__\n",
    "\n",
    "*Keep it in mind when deciding how to implement an optimization problem in Machine Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1uy-iAYxVsJ"
   },
   "source": [
    "What happens if we modify $\\alpha$?\n",
    "\n",
    "Rembember that the GD method has __sublinear convergence rate depending on $\\alpha$__, meaning that:\n",
    "1) The bigger $\\alpha$, the faster we have convergence... __BUT__\n",
    "2) If $\\alpha$ is too big, the gradient explodes!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __TRY IT YOURSELF__\n",
    "Change step size (eta) and observe what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bq4BhDF3Ykw"
   },
   "source": [
    "What if we have a more complex function?\n",
    "\n",
    "What if we want to choose a more complex algorithm, such as BFGS or Conjugate Gradient?\n",
    "\n",
    "Here we have __Scipy__\n",
    "\n",
    "Documentation: https://docs.scipy.org/doc/scipy/reference/optimize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1679497988091,
     "user": {
      "displayName": "Cecilia Salvatore",
      "userId": "06475737550872053301"
     },
     "user_tz": -60
    },
    "id": "DBRCdVvd3bWx",
    "outputId": "6ecc782d-6d10-46c8-cc5e-56511566dca8"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "x0 = np.random.random((2,))\n",
    "\n",
    "res = minimize(\n",
    "    fun=f, # OBJECTIVE FUNCTION TO OPTIMIZE\n",
    "    x0=x0, # STARTING POINT\n",
    "    method='BFGS', # OPTIMIZER\n",
    "    jac=df_dx # GRADIENT FUNCTION\n",
    ")\n",
    "\n",
    "#NOTE that if we do not specify the jac argument, the gradient will be estimated automatically but it will be less precise\n",
    "print(res.message)\n",
    "\n",
    "print(f\"Time for optimizing: {time.time()-start} seconds\")\n",
    "print(f\"Solution {res.x}, Objective function {f(res.x)}, Gradient value {np.linalg.norm(df_dx(res.x))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrlRIT5n4oYV"
   },
   "source": [
    "Let's finally see what happens if we want to optimize a __function that depends also on parameters__.\n",
    "\n",
    "This case is very frequent in Machine Learning, for example when you are optimizing an error function that depends also on the __hyper-parameters__, such as the regularization term.\n",
    "\n",
    "Imagine you want to optimize\n",
    "$$ f(x) = a_1 x_1^2 + a_2 x_2^2\n",
    "$$\n",
    "\n",
    "Where $x \\in \\mathbb{R}^2$ is the argument of the function, while $a_1,a_2 \\in \\mathbb{R}$ are two given parameters.\n",
    "\n",
    "In this case we can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1679498154699,
     "user": {
      "displayName": "Cecilia Salvatore",
      "userId": "06475737550872053301"
     },
     "user_tz": -60
    },
    "id": "sax2e1Of46fF"
   },
   "outputs": [],
   "source": [
    "def f(X, function_args) -> np.ndarray:  # X is a vector. X MUST BE the FIRST ARGUMENT!!!\n",
    "    a1 = function_args[0] # unwrapping the arguments\n",
    "    a2 = function_args[1]\n",
    "    x1 = X[0] # unwrapping the variable\n",
    "    x2 = X[1]\n",
    "    return a1*x1**2 + a2*x2**2\n",
    "\n",
    "def grad_f(X, function_args) -> np.ndarray:\n",
    "    a1 = function_args[0] # unwrapping the arguments\n",
    "    a2 = function_args[1]\n",
    "    x1 = X[0] # unwrapping the variable\n",
    "    x2 = X[1]\n",
    "    d1 = 2*a1*x1\n",
    "    d2 = 2*a2*x2\n",
    "    return np.array([d1,d2])\n",
    "\n",
    "#NOTE: function and graidient must take the same arguments as input!\n",
    "\n",
    "#We define a starting point and two values for a1 and a2\n",
    "x0 = np.array([1,3])\n",
    "\n",
    "a1 = 1\n",
    "a2 = 2\n",
    "\n",
    "function_args = [a1,a2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1679498215282,
     "user": {
      "displayName": "Cecilia Salvatore",
      "userId": "06475737550872053301"
     },
     "user_tz": -60
    },
    "id": "fN_Lustz5AlQ",
    "outputId": "533e8836-92dc-4bb0-f43a-11bbc9d945c7"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "res = minimize(f, x0, args=function_args,jac=grad_f,method='bfgs')\n",
    "\n",
    "print(res.message)\n",
    "print(f\"Time for optimizing: {time.time()-start} seconds\")\n",
    "print(f\"Solution {res.x}, Objective function {f(res.x, function_args)}, Gradient value {np.linalg.norm(grad_f(res.x, function_args))}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOlF1XpgJgSD4RWe/ExKaRW",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
